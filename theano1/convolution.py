
import theano
from theano import tensor as T
from theano.tensor.nnet import conv

import numpy

# There is an image in 'RGB' and we want to apply 'two' randomly initialized 9x9 filters as convolution on it and see the result.

rng = numpy.random.RandomState(23455)


####################           Symbolic Model Construction
######################################################################
######################################################################
######################################################################
######################################################################
inputMB = T.tensor4(name='inputMB')  # initialize the input minibatch as a 4D matrix. (4D tensor for input image mini-batch.) TensorType(dtype, (False, False, False, False))


# I have 2 filters each 9x9 pixels and one per color (RGB) so overall 2x3x9x9 parameters
w_shape = (2, 3, 9, 9)
w_bound = numpy.sqrt(3 * 9 * 9)  # uniform distribution in the range [-1/fan-in, 1/fan-in]
# initialize filters (weights) randomly.  (as a theano shared variable)
W = theano.shared( numpy.asarray(
            rng.uniform(
                low =-1.0 / w_bound,
                high= 1.0 / w_bound,
                size=w_shape),
            dtype=inputMB.dtype), name ='W')

# symbolic expression to compute the convolution of inputMB with filters in W
# nnet.conv2d will build the symbolic graph for convolving a stack of input images with a set of filters. 
#             The implementation is modelled after Convolutional Neural Networks (CNN). It is simply a wrapper to the ConvOp but provides a much cleaner interface.
#             params:
#                 - input (symbolic 4D tensor) - mini-batch of feature map stacks, of shape (batch size, stack size, nb row, nb col) see the optional parameter image_shape
#                 - filters (symbolic 4D tensor) - set of filters used in CNN layer of shape (nb filters, stack size, nb row, nb col) see the optional parameter filter_shape
#             returns:
#                 - set of feature maps generated by convolutional layer. Tensor is of shape (batch size, nb filters, output row, output col)      ( no stack size)
conv_out = conv.conv2d(inputMB, W)   # http://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv.conv2d





# initialize bias randomly (1D tensor - one bias per filter, we have two filters). (as a theano shared variable)
# IMPORTANT: biases are usually initialized to zero. However in this particular application, we simply apply the convolutional layer to an image without learning the parameters. 
# We therefore initialize them to random values to "simulate" learning.
b_shape = (2,)    # To write a tuple containing a single value you have to include a comma, even though there is only one value:     tup1 = (50,);
b = theano.shared(numpy.asarray(
            rng.uniform(low=-.5, high=.5, size=b_shape),
            dtype=inputMB.dtype), name ='b')


# we use dimshuffle to add missing dimensionalities at runtime (we can do this because this is a symbolic varibale) 
# as it is also a numpy array it specifies broadcastable dimensions
# it's broadcastable is as follows: (True, False, True, True) , so we essentially can broadcast b on it's missing dimensions.
# dimensions with size 1 are stretched or "copied" to match the other. http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html
    #   ``TensorVariable.dimshuffle(*pattern)`` Returns a view of this tensor with permuted dimensions. 
        #    Typically the pattern will include the integers 0, 1, ... ndim-1, and any number of 'x' characters in dimensions where this tensor should be broadcasted. 
        #    Shuffle dimensions around, and insert new ones along which the tensor will be broadcastable;
        #    
        #   More examples:
        #    dimshuffle('x') -> make a 0d (scalar) into a 1d vector
        #    dimshuffle(0, 1) -> identity
        #    dimshuffle(1, 0) -> inverts the first and second dimensions
        #    dimshuffle('x', 0) -> make a row out of a 1d vector (N to 1xN)
        #    dimshuffle(0, 'x') -> make a column out of a 1d vector (N to Nx1)
        #    dimshuffle(2, 0, 1) -> AxBxC to CxAxB
        #    dimshuffle(0, 'x', 1) -> AxB to Ax1xB
        #    dimshuffle(1, 'x', 0) -> AxB to Bx1xA
        #    dimshuffle('x', 2, 'x', 0, 1)    AxBxC tensor is mapped to 1xCx1xAxB 
b_shuff = b.dimshuffle('x', 0, 'x', 'x')   # TODO: Why the second dimension?

# Symbolic expression to add bias and then apply activation function (i.e. produce neural net layer output)
output = T.nnet.sigmoid(conv_out + b_shuff)

# create theano function to process inputs to outputs
f = theano.function([inputMB], output)

####################           Apply model on data
######################################################################
######################################################################
######################################################################
######################################################################

import pylab
import PIL

# open random image of dimensions 639x516
img = PIL.Image.open(open('resources/3wolfmoon.jpg'))  # @UndefinedVariable
# dimensions are (height, width, channel)
img = numpy.asarray(img, dtype='float64') / 256.   # (639, 516, 3)

# put image in 4D tensor of shape (1, 3, height, width)   # as a single item batch
# numpy.transpose Permute the dimensions of an array.
img_t = img.transpose(2, 0, 1)         # (3, 639, 516)
img_ = img_t.reshape(1, 3, 639, 516)   # (1, 3, 639, 516)
filtered_img = f(img_)  #   (1, 2, 631, 508)  a 9x9 filter removes  pixels from each side
# TODO: what happens to the colors? there is no notion of stack size in output of conv

# plot original image and first and second components of output
pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(img)
# recall that the convOp output (filtered image) is actually a "minibatch",
# of size 1 here, so we take index 0 in the first dimension:
conv1_result = filtered_img[0, 0, :, :]
conv2_result = filtered_img[0, 1, :, :]
pylab.gray();
pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(conv1_result)
pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(conv2_result)
pylab.show()

